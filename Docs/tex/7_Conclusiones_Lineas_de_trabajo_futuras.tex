\capitulo{7}{Conclusiones y Líneas de trabajo futuras}

Para este proyecto entrenaron numerosos modelos hasta quedarme con los detallados anteriormente. Creo no equivocarme si digo que la mejor forma de evaluar estos modelos es de forma empírica, es decir, observando cómo juegan y qué resultados obtienen.

Por un lado tenemos los clasificadores. Se entrenaron tres modelos, uno con Decision Tree simple, otro con Random Forest y un tercero que se trataba de un Decision Tree aleatorio (con datos de entrada barajado). Estaba seguro de que el mejor iba a funcionar era el Random Forest, un tipo de clasificador mas sofisticado que su contrincante. Contradiciendo a mi hipótesis inicial, el que mejor funcionó fue el Decision tree y analizándolo detenidamente tiene sentido. Cuando tenemos un conjunto de datos no muy grande, como fue mi caso, no conviene crear estados muy concretos si no que conviene generalizar mucho mas y dejar hueco a la improvisación. En otras palabras, el random forest sobreajustaba la función. Para mi sorpresa, los dos clasificadores mencionados no distaban mucho del modelo entrenado aleatoriamente, lo que hace plantearme varias cosas. ¿Son el random forest y el Decision tree malos algoritmos para el caso que nos ocupa?, ¿El conjunto de datos era insuficiente y le hace comportarse como si de un aleatorio se tratase? ¿Mi aleatorio es tremendamente afortunado? Para resolver estas dudas había que probar con un dataset mucho mas grande y generar varios modelos más de clasificadores aleatorios, pero había que pasar a la parte evolutiva.


Por otro lado tenemos el perceptrón multicapa con una red neuronal convolucional, el claro vencedor. Este algoritmo comienza también con un set de datos de entrada que le servirán como referencia para la tarea de imitación y a continuación se le hace evolucionar. Bien es cierto que ésta neuroevolución ha empleado más de cuatro horas para llegar donde ha llegado, pero ha merecido la pena. En las primera generaciones se podría decir que imitaba bien, de una forma aceptable, al humano pero no se diferenciaba notablemente de los clasificadores empleados en la etapa anterior. Según fueron pasando generaciones el modelo se fue alejando de ese estilo de juego para pasar a una estrategia más mecánica, descubriendo un error que cometí a la hora de generar los enemigos. este error hace que el punto de partida de mi generador de enemigos los genere siempre en un punto aleatorio de la mitad izquierda de la pantalla, por lo que en el lado derecho nunca se generan. El modelo final centra todos sus disparos precisamente en esta zona, dejando la zona derecha prácticamente sin visitar. 

Por lo tanto, una vez analizados los resultados, podemos concluir que teniendo en cuenta la habilidad del modelo entrenado el vencedor es el Perceptrón multicapa, ya que ha obtenido las máximas puntuaciones. Si tenemos en cuenta la forma de comportarse del modelo ante los estímulos, los del perceptrón se ven muy mecánicos, no parecen humanos, por lo que yo me decantaría por del Decision tree, que el mas  ``humano'' aunque mucho mas torpe.


Cuando se comenzó a trabajar en este proyecto ya tenia conocimiento de ciertos aspectos del aprendizaje máquina, pero desconocía totalmente cómo se había de llevar a cabo y qué herramientas existían para ello. Fue una gran noticia descubrir las librerías de scikit-lean y su relativa simplicidad de uso. Debido a este desconocimiento, tuve que dedicar mucho tiempo a invertigación y por desgracia se me quedaron algunas cosas en el tintero. 

El juego tiene implementados dos tipos de power-up, o potenciadores que mejoran temporalmete las capacidades de la nave del jugador. La intención era que fuesen apareciendo aleatoriamente durante la partida y el jugador interactuase ellos. Finalmente solo de dejaron los power-up que aparecen al eliminar al jefe de final de nivel.Otro punto interesante es dotar a los enemigo de cierto poder ofensivo, ya que actualmente solo avanzan de forma pasiva. Creo que el hecho de que los enemigos ataquen da un punto interesante de complejidad.